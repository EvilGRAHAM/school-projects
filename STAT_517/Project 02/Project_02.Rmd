---
title: "Does Size Matter? (Estimation of Banana Weight with a Regression Modeling Approach)"
author: "Scott Graham, Kaisa Roggeveen"
date: "February 13, 2018"
header-includes:
  - \newcommand{\Prob}{\operatorname{P}}
  - \newcommand{\E}{\operatorname{E}}
  - \newcommand{\Var}{\operatorname{Var}}
  - \newcommand{\Cov}{\operatorname{Cov}}
  - \newcommand{\se}{\operatorname{se}}
  - \newcommand{\re}{\operatorname{re}}
  - \newcommand{\ybar}{{\overline{Y}}}
  - \newcommand{\phat}{{\hat{p}}}
  - \newcommand{\that}{{\hat{T}}}
  - \newcommand{\med}{{\tilde{Y}}}
  - \newcommand{\lnit}{{\operatorname{Logit}}}
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(pander, warn.conflicts = FALSE, quietly = TRUE)
library(MASS, warn.conflicts = FALSE, quietly = TRUE)
library(DAAG, warn.conflicts = FALSE, quietly = TRUE)
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE)
library(magrittr, warn.conflicts = FALSE, quietly = TRUE)
library(ggfortify, warn.conflicts = FALSE, quietly = TRUE)
library(knitr, warn.conflicts = FALSE, quietly = TRUE)

set.seed(5609)

theme_minimal2 <- theme_minimal() %>%  theme_set()
theme_minimal2 <-
  theme_update(
    panel.border = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
    ,strip.background = element_rect(
      linetype = "solid"
      ,colour = "grey92"
        ,fill = NA
      )
    )

banana_data <-
  "mybanana.txt" %>% 
  read_tsv()
banana_data <-
  banana_data %>% 
  mutate_at(
    .vars = vars(Weight:Circumference)
    ,.funs = funs(log = log)
  )

banana_tidy <- 
  banana_data %>% 
  select(
    -c(
      Weight_log
      ,Radius_log
      ,Length_log
      ,Circumference_log
    )
  ) %>% 
  gather(
    key = "Type"
    ,value = "Measurement"
    ,-ID
  )
```



# Summary


# Introduction
The purpose of this study was to determine the most effective regression model to predict the weight of a banana using external measurements. This study also demonstrated multiple techniques for developing regression models. These models were then examined to demonstrate their effectiveness at creating regression models.

For the purpose of this study, it is assumed $\alpha=0.05$ for all tests done.



# Data Collection
First a small sample set bananas were purchased from the Real Canadian Superstore. The weight, length, diameter and circumference were then calculated using a scale and a ruler. 

```{r Sample Size, cache=TRUE}
rsquare_sim <- c()
for(i in 1:1000){
  banana_cor <- 
    banana_data %>%
    sample_n(10) %>% 
    select(
      Weight
      ,Radius
      ,Length
      # ,Circumference
    ) %>% 
    cor()
  
  xy_vec <- banana_cor[2:3, 1]
  C_mat <- banana_cor[2:3, 2:3]
  
  rsquare_sim[i] <- t(xy_vec) %*% solve(C_mat) %*% xy_vec
}
rsquare_sim %>% 
  summary() %>% 
  pander(caption = "Summary Statisitcs for Simulated R-Squared")
```

In order to determine the minimum sample size needed, random sample sizes of 10 were generated using radius and length as the predictors. The correlation of the random sample sizes were calculated and a matrix of the correlations were generated. The value of the squared population multiple correlation coefficients with two predictor variables was then calculated and determined to be approximately `r round(mean(rsquare_sim), 4)`. From this the minimum sample size required was then determined from the table from Gregory T. Knofcznski's Sample Size When Using Multiple Linear Regression for Prediction, the minimum sample size was determined to be between 15 and 35, therefore the minimum number of bananas required was finalized at 24 bananas. 



# Analysis
## Preliminary Analysis
```{r Summary Stats}
banana_summary <-
  cbind(
    Statistic = 
      c(
        "Min."
        ,"1st Qu."
        ,"Median"
        ,"Mean"
        ,"3rd Qu."
        ,"Max"
      )
    ,banana_data %>% 
      select(
        -c(
          ID
          ,Weight_log
          ,Radius_log
          ,Length_log
          ,Circumference_log
        )
      ) %>% 
      map_df(summary)
  ) %>% 
  as.tibble()
kable(banana_summary, caption = "Banana Summary Statisitcs")

banana_tidy %>% 
  ggplot(aes(x = Measurement, colour = Type)) +
  geom_histogram(
    aes(y = ..density..)
    ,alpha = 0
    ,binwidth = function(x) nclass.FD(x)
  ) +
  geom_density() +
  facet_wrap(
    ~ Type
    ,scales = "free"
  ) +
  scale_colour_brewer(
    palette = "Dark2"
    ,type = "qual"
  ) +
  labs(
    title = "Figure 01: Sample Distributions of Banana Data"
    ,y = "P(Y=y)"
  ) +
  theme(legend.position = "none")
```

```{r Visualizations}
banana_data %>% 
  select(
    -c(
      Weight_log
      ,Radius_log
      ,Length_log
      ,Circumference_log
    )
  ) %>% 
  gather(
    key = "Type"
    ,value = "Measurement"
    ,-ID
    ,-Weight
  ) %>% 
  ggplot(
    aes(
      x = Measurement
      ,y = Weight
      ,colour = Type
    )
  ) +
  geom_smooth(
    method = "loess"
    ,se = FALSE
  ) +
  geom_smooth(
    method = "lm"
    ,se = FALSE
  ) +
  geom_point() +
  facet_wrap(
    ~ Type
    ,scales = "free_x"
  ) +
  scale_colour_brewer(
    palette = "Set2"
    ,type = "qual"
  ) +
  labs(
    title = "Figure 02: Weight vs. Predictors"
    ,y = "Weight (g)"
  ) +
  theme(legend.position = "none")
```


## Initial Regression Models
To begin analysis, a model using all predictor variables was created. In this case the density of the banana is assumed to be a constant. In the following models all measured bananas were considered. 

Let:
$$
  W = \text{Weight (g), }
  L = \text{Length (mm), }
  R = \text{Radius (mm), }
  C = \text{Circumference (mm)}
$$
Then:
\begin{equation}
  \ln(W) = 
  \beta_{0} + \beta_{1}\ln(L) + \beta_{2}\ln(R) + \beta_{3}\ln(C) \implies
  W = 
  e^{\beta_{0}} \times L^{\beta_{1}} \times R^{\beta_{2}} \times C^{\beta_{3}}
\end{equation}

```{r Reg 01}
banana_reg_01 <-
  banana_data %>% 
  lm(
    Weight_log ~ Length_log + Radius_log + Circumference_log
    ,data = .
  )
pander(summary(banana_reg_01))
```

None of the predictor variables were found to be statistically significant. This is due to the high degree of collinearity exhibited between Radius and Circumference. As such, in the second model, the predictor variable, circumference, was removed. This is because $C=2\pi R$, leading to collinearity. This can also be seen by examining the correlation plot produced by the variables:
```{r Cor Plot}
banana_data %>%
  select(
    -c(
      ID
      ,Weight_log
      ,Radius_log
      ,Length_log
      ,Circumference_log
    )
  ) %>% 
  cor() %>% 
  as.data.frame() %>%  
  rownames_to_column() %>% 
  as.tibble() %>% 
  gather(
    key = Column
    ,value = Correlation
    ,-rowname
  ) %>% 
  rename(Row = rowname) %>% 
  ggplot(
    aes(
      x = Column
      ,y = Row
      ,fill = Correlation
    )
  ) +
  geom_raster() +
  scale_fill_distiller(
    type = "div"
    ,palette = "RdBu"
    ,limits = c(-1, 1)
  ) +
  labs(title= "Figure 03: Correlation Plot") +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1)
    ,axis.title.x = element_blank()
    ,axis.title.y = element_blank()
    ,panel.grid = element_blank()
  )
```

The second model considered only the predictor variables length and radius:
\begin{equation}
  \ln(W) = \beta_{0} + \beta_{1}\ln(L) + \beta_{2}\ln(R)
\end{equation}

```{r Reg 02}
banana_reg_02 <-
  banana_data %>% 
  lm(
    Weight_log ~ Length_log + Radius_log
    ,data = .
  )
pander(summary(banana_reg_02))
```

The third model considered the predictor, length.
\begin{equation}
  \ln(W) = \beta_{0} + \beta_{1}\ln(L)
\end{equation}

```{r Reg 03}
banana_reg_03 <-
  banana_data %>% 
  lm(
    Weight_log ~ Length_log
    ,data = .
  )
pander(summary(banana_reg_03))
```

The fourth model considered only one predictor, radius.
\begin{equation}
  \ln(W) = \beta_{0} + \beta_{2}\ln(R)
\end{equation}

```{r Reg 04}
banana_reg_04 <-
  banana_data %>% 
  lm(
    Weight_log ~ Radius_log
    ,data = .
  )
pander(summary(banana_reg_04))
```

ANOVAs were then done to determine if a statistically significant difference in levels of explained variation existed between the 4 models. 1st, 01 and 02 were compared, testing the effect of Circumference:
```{r ANOVA 01 vs 02}
pander(anova(banana_reg_02, banana_reg_01), caption = "Analysis of Variance Table: Model 01 vs. Model 02")
```

Based on the data, we failed to reject the null hypothesis of equal explained variance between the two models. As such, we believe the model 02 is more appropriate, as it is smaller and more parsimonious.

The following test compares models 02 and 03, testing the effect of Radius:
```{r ANOVA 02 vs 03}
pander(anova(banana_reg_03, banana_reg_02), caption = "Analysis of Variance Table: Model 02 vs. Model 03")
```

Based on the data, we rejected the null hypothesis of equal explained variance between the two models. As such, we believe the model 02 is more appropriate, as it does a better job at explaining the underlying variance in the data.

The following test compares models 02 and 03, testing the effect of Length:
```{r ANOVA 02 vs 04}
pander(anova(banana_reg_04, banana_reg_02), caption = "Analysis of Variance Table: Model 02 vs. Model 04")
```

Based on the data, we failed to reject the null hypothesis of equal explained variance between the two models. As such, we believe the model 04 is more appropriate, as it is smaller and more parsimonious.

From this, we found the most appropriate model was model 04, as it is the smallest model that explains an adequate amount of variance. This model being:
$$
  \ln(W) =
  0.3046 + 1.6690\ln(R) \implies
  W =
  e^{0.3046}R^{1.6690}
$$
This is an unexpected result, as it doesn't include the Length predictor, implying that the weight of a banana is purely a function of its radius. Due to this, further analysis was required.

## Removal of Outliers
Due to the surprising results obtained above, the choice to check for potential outliers was made. For this, model 02 was chosen, as we wished to examine the potential presence of outlier with respect to both Length and Radius, which model 04 would fail to accomplish.

```{r Outlier Check}
banana_resid_data <- 
  tibble(
    Predicted = predict(banana_reg_02)
    ,Actual = banana_data$Weight_log
    ,ID = banana_data$ID
    ,`Std Residuals` = stdres(banana_reg_02)
    ,Leverage = hatvalues(banana_reg_02)
  ) %>% 
  mutate(Residual = Actual - Predicted)

banana_resid_data %>% 
  ggplot(aes(x = Predicted, y = `Std Residuals`)) +
  geom_hline(
    aes(yintercept = -2)
    ,linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 2)
    ,linetype = "dashed"
  ) +
  geom_point() +
  geom_text(
    data =
      banana_resid_data %>% 
      filter(abs(`Std Residuals`) >= 2)
    ,aes(label = ID)
    ,nudge_x = 0.005
  ) +
  geom_smooth(
    method = "loess"
    ,se = FALSE
  ) +
  geom_smooth(
    method = "lm"
    ,se = FALSE
  ) +
  labs(
    title = "Figure 04: Standardized Residuals vs. Predicted for Model 02"
    ,x = "Predicted (ln)"
    ,y = "Standardized Residual (ln)"
  )

banana_resid_data %>% 
  ggplot(aes(x = Leverage, y = `Std Residuals`)) +
  geom_hline(
    aes(yintercept = -2)
    ,linetype = "dashed"
  ) +
  geom_hline(
    aes(yintercept = 2)
    ,linetype = "dashed"
  ) +
  geom_point() +
  geom_text(
    data =
      banana_resid_data %>% 
      filter(abs(`Std Residuals`) >= 2)
    ,aes(label = ID)
    ,nudge_x = 0.02
  ) +
  geom_smooth(
    method = "loess"
    ,se = FALSE
  ) +
  geom_smooth(
    method = "lm"
    ,se = FALSE
  ) +
  labs(
    title = "Figure 05: Standardized Residuals vs. Leverage for Model 02"
    ,x = "Leverage (ln)"
    ,y = "Standardized Residual (ln)"
  )

banana_data %>% 
  inner_join(
    banana_resid_data %>% 
      filter(abs(`Std Residuals`) > 2)
    ,by = "ID"
  ) %>% 
  select(
    ID
    ,Weight
    ,Radius
    ,Length
    ,Circumference
    ,`Std Residuals`
    ,Leverage
  ) %>% 
  kable(caption = "Entries with a |Standardized Residual| >2")
```

By looking at values with $|\text{Std. Residual}|>2$, we can identify potential outliers. Then, by examining the leverage of those outliers we can make a determination of whether or not they are severely distorting the regression line from the true slope. Although both 6 and 11 have a large standardized residual, only 6 has a large leverage associated with it, and as such is excluded from the dataset.


## Cross Validation
```{r CV}
banana_data_post <-
  banana_data %>% 
    inner_join(
    banana_resid_data
    ,by = "ID"
  ) %>% 
  filter(
    !(abs(`Std Residuals`) > 2 & Leverage > 0.2)
  ) %>%
  select(
    -c(
      Predicted
      ,Actual
      ,`Std Residuals`
      ,Leverage
      ,Residual
    )
  )

banana_reg_cv <-
  banana_data_post %>%
  cv.lm(
    Weight_log ~ Length_log + Radius_log
    ,plotit = FALSE
  )

banana_reg_cv %>% 
  pander(caption = "CV Model")
```

### MAE
```{r MAE}
tibble(
  MSE =
    (
      banana_reg_cv %>% 
        transmute((Weight_log - cvpred)^2) %>% 
        sum()
    )/(as.numeric(count(banana_reg_cv)))
  ,MAE =
    (
      banana_reg_cv %>% 
        transmute(abs(Weight_log - cvpred)) %>% 
        sum()
    )/(as.numeric(count(banana_reg_cv)))
  ,MPAE =
    (
      banana_reg_cv %>% 
        transmute(abs((Weight_log - cvpred) / Weight_log)) %>% 
        sum()
    )/(as.numeric(count(banana_reg_cv)))
) %>% 
  kable(caption = "Calculated Error Terms for CV Model")
```



# Recommendations
Using the first set of data before the outlier was removed, it can be determined that the best way to predict the weight of a banana is by measuring the radius of the banana. The model that is then used for banana weight prediction is the following:
$$
  \ln(W) = \beta_{0} + \beta_{1}\ln(R)
$$
$$
  \ln(W) = 0.3046 + 1.669\ln(R)
$$
After the removal of the outlier, the model that was determined to be the best predictor for banana weight was the following:



# Appendix
## Appendix A: Code
```{r Appendix A, eval=FALSE, echo=TRUE}
library(pander, warn.conflicts = FALSE, quietly = TRUE)
library(MAAS, warn.conflicts = FALSE, quietly = TRUE)
library(DAAG, warn.conflicts = FALSE, quietly = TRUE)
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE)
library(magrittr, warn.conflicts = FALSE, quietly = TRUE)
library(ggfortify, warn.conflicts = FALSE, quietly = TRUE)
library(knitr, warn.conflicts = FALSE, quietly = TRUE)

set.seed(5609)

theme_minimal2 <- theme_minimal() %>%  theme_set()
theme_minimal2 <-
  theme_update(
    panel.border = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
    ,strip.background = element_rect(
      linetype = "solid"
      ,colour = "grey92"
        ,fill = NA
      )
    )

banana_data <-
  "mybanana.txt" %>% 
  read_tsv()

banana_data <-
  banana_data %>% 
  mutate_at(
    .vars = vars(Weight:Circumference)
    ,.funs = funs(log = log)
  )

banana_tidy <- 
  banana_data %>% 
  select(
    -c(
      Weight_log
      ,Radius_log
      ,Length_log
      ,Circumference_log
    )
  ) %>% 
  gather(
    key = "Type"
    ,value = "Measurement"
    ,-ID
  )
```