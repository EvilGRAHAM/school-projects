---
title: "Leafs"
author: "Kaisa Roggeveen, Scott Graham"
date: "March 22nd 2018"
header-includes:
  - \newcommand{\Prob}{\operatorname{P}}
  - \newcommand{\E}{\operatorname{E}}
  - \newcommand{\Var}{\operatorname{Var}}
  - \newcommand{\Cov}{\operatorname{Cov}}
  - \newcommand{\se}{\operatorname{se}}
  - \newcommand{\re}{\operatorname{re}}
  - \newcommand{\ybar}{{\overline{Y}}}
  - \newcommand{\phat}{{\hat{p}}}
  - \newcommand{\that}{{\hat{T}}}
  - \newcommand{\med}{{\tilde{Y}}}
  - \newcommand{\logit}{{\operatorname{Logit}}}
output: 
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(pander, warn.conflicts = FALSE, quietly = TRUE)
library(knitr, warn.conflicts = FALSE, quietly = TRUE)
library(MASS, warn.conflicts = FALSE, quietly = TRUE)
library(pROC, warn.conflicts = FALSE, quietly = TRUE)
library(caret, warn.conflicts = FALSE, quietly = TRUE)
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE)
library(magrittr, warn.conflicts = FALSE, quietly = TRUE)
library(ggfortify, warn.conflicts = FALSE, quietly = TRUE)

theme_minimal2 <- theme_minimal() %>%  theme_set()
theme_minimal2 <-
  theme_update(
    panel.border = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
    ,strip.background = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
  )

# Functions ----------
geom_cor <- function(data, ...){
  data %>%
    filter_all(any_vars(!is.na(.))) %>% 
    cor(...) %>% 
    as.data.frame() %>%  
    rownames_to_column() %>% 
    as.tibble() %>% 
    gather(
      key = Column
      ,value = Correlation
      ,-rowname
    ) %>% 
    rename(Row = rowname) %>% 
    ggplot(
      aes(
        x = Column
        ,y = Row
        ,fill = Correlation
      )
    ) +
    geom_raster() +
    scale_fill_distiller(
      type = "div"
      ,palette = "RdBu"
      ,limits = c(-1, 1)
    ) +
    theme(
      axis.text.x = element_text(angle = 90, hjust = 1)
      ,axis.title.x = element_blank()
      ,axis.title.y = element_blank()
      ,panel.grid = element_blank()
      ,panel.background = element_blank()
    )
}

ggroc <- function(roc, showAUC = TRUE, interval = 0.2, breaks = seq(0, 1, interval)){
  require(pROC)
  if(class(roc) != "roc") simpleError("Please provide roc object from pROC package")
  
  plot_data <- 
    data.frame(
      plotx <- rev(roc$specificities)
      ,ploty <- rev(roc$sensitivities)
    )
  
  plot_data %>% 
    ggplot(
      aes(
        x = plotx
        ,y = ploty
      )
    ) +
    geom_segment(
      aes(
        x = 0
        ,y = 1
        ,xend = 1
        ,yend = 0
      )
      ,alpha = 0.5
    ) + 
    geom_step() +
    scale_x_reverse(
      name = "Specificity"
      ,limits = c(1, 0)
      ,breaks = breaks
      ,expand = c(0.001,0.001)
    ) + 
    scale_y_continuous(
      name = "Sensitivity"
      ,limits = c(0, 1)
      ,breaks = breaks
      ,expand = c(0.001, 0.001)
    ) +
    coord_equal() + 
    annotate(
      geom = "text"
      ,x = 0.05 + interval/2
      ,y = interval/2
      ,vjust = 0
      ,label = paste("AUC =", sprintf("%.3f",roc$auc))
    )
}

# Data Import ----------
leaf_data <- 
  "../Data/leaf_data.csv" %>% 
  read_csv() %>% 
  mutate(Type = as.factor(Type))

leaf_data$Type <- factor(leaf_data$Type, levels = c("Pear", "Cherry"))

measurements <- c("Length", "Width")

leaf_test <-
  tibble(
    Number = 1:3
    ,Length = c(8.2, 5.2, 7.6)
    ,Width = c(3.2, 3.8, 4.0)
  )
```
Dr. Steven M. Vamosi
Associate Dean, Diversity, Equity and Inclusion
Professor, Population Biology
2500 University Drive NW
Department of Biological Sciences
University of Calgary
Calgary AB
T2N 1N4 Canada

# Introduction
The intent of this paper is to develop a method for classifying leaves as either Cherry or Pear, based on their measured length and width. This method was developed for Dr. Steven Vamosi, a botanist from the University of Calgary. 

The classification method used was Linear discriminant analysis, developed by R.A Fischer. To .... To take training samples from a sampled population take measurements, from these measurements classification rules are created. This will then be tested against the classifying sample to see if there are any miss classifications.

Cherry and Pear leaves are both leaves from fruit trees. Cherry trees belong to the genus Prunus and Pear trees belong to the genus Pyrus [2],[3]. A common feature amongst the leaves is that they both have a midrib, which is the central vein of the leaf which extends along the leaf's center line. 


# Data
## Measurement Process
The first step taken in the measurement of the leaves was to give each leaf an identification number based on the species. The method used to measure the dimensions was to create a box with the minimum length and width in which the entire leaf would be encompassed in the box. 

To begin creating the sides of the box, a ruler was aligned parallel to the midrib, which is the central vein in the leaf and moved towards the left and the right of the picture until only one point on the leaf remained [1]. From the single point on the side of the leaf, a line was drawn parallel to the midrib of the leaf. 

Next, the base and point of the leaf were measured, a ruler was placed perpendicular to the midrib and the ruler was moved towards to tip of the leaf until a single point remained, a line was draw perpendicular to the midrib at this point. At the base of the leaves the length of the leaf was set as the point where the leaf ends and the stem begins, at this point a line was drawn perpendicular to the midrib. 

After all the boxes were created, the width (lines parallel to midrib) and the length (lines perpendicular to midrib) were measured and the results were recorded in a spread sheet. 

## Data Creation
```{r Data Creation 01}
leaf_data %>% 
  select(-starts_with("Number")) %>% 
  group_by(Type) %>% 
  summary() %>% 
  kable(caption = "Data Summary")
```

In this original data set there are a few issues that need to be acknowledged. The first issues that occurred during the data measurements was the result of the leaves that were distributed as the training sample were images, in which the images were not to scale. This resulted in a few outlines, which much larger lengths and widths compared to the other leaves in the set. These outlines included Pear#12, Cherry#10 and Cherry#5. However, based on the nature of this project in just observing the ratio between the length and width, this should not be affected by the size of the image, unless the image was stretched in either direction.

```{r Scatter Plot}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Type
      ,label = `Number By Type`
    )
  ) + 
  geom_text() +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 01: Length vs Width Scatter Plot"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```
In Figure 01 there is a distinct separation in the data of the cherry and the pear. As mentioned above, the outlines are Cherry#10, Cherry#5 and Pear#12 these outlines appear to follow a similar grouping and therefore they were kept in the data set. The raw data is located in Appendix A.  

# Classification Procedure (LDA)
## Training Data
```{r LDA}
leaf_lda <- 
  leaf_data %>%
  lda(
    Type ~ Length + Width
    ,data = .
    ,cv = TRUE
  )

leaf_lda$prior %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  kable(
    digits = 4
    ,col.names = c("Type", "Probability")
    ,caption = "LDA Prior Probabilities"
  )

leaf_lda$mean %>% 
  kable(
    digits = 4
    ,caption = "LDA Group Means"
  )

leaf_lda$scaling %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  kable(
    digits = 4
    ,col.names = c("Dimension", "Coefficient")
    ,caption = "LDA Coefficients of Linear Discriminants"
  )

leaf_lda_pred <- predict(leaf_lda, newdata = select(leaf_data, Length, Width))

leaf_lda_pred$class <- as.vector(leaf_lda_pred$class)
leaf_lda_pred$posterior <- as_tibble(leaf_lda_pred$posterior)
leaf_lda_pred$x <- as.vector(leaf_lda_pred$x)

leaf_lda_tidy <-
  tibble(
    Predicted = factor(leaf_lda_pred$class, levels = c("Pear", "Cherry"))
    ,Cherry = leaf_lda_pred$posterior$Cherry
    ,Pear = leaf_lda_pred$posterior$Pear
    ,LD1 = leaf_lda_pred$x
  ) %>% 
  bind_cols(leaf_data) %>% 
  rename(
    Actual = Type
    ,`Number By Actual` = `Number By Type`
  ) %>%
  group_by(Predicted) %>% 
  mutate(
    `Number By Predicted` = 1:length(Predicted)
  ) %>% 
  ungroup()
```

Tables 2-4 represent the output of a linear discriminant analysis (LDA) done on the raw data. Table 2 represents the prior probabilities of falling in a particular type. The prior probability of being a Pear leaf is `r round(leaf_lda$prior[["Pear"]], 4)`. The prior probability of being a Cherry leaf is `r round(leaf_lda$prior[["Cherry"]], 4)`. 

NOTE INCLUDE LDA COEFFICIENT MEANINGS

```{r LDA Results}
leaf_lda_tidy %>% 
  select(
    Predicted
    ,Actual
    ,Length
    ,Width
    ,`Cherry Probability` = Cherry
    ,`Pear Probability` = Pear
  ) %>% 
  mutate(`Correct Prediction` = if_else(Predicted == Actual, TRUE, FALSE)) %>% 
  filter(!`Correct Prediction`) %>% 
  kable(digits = 4, caption = "LDA Misclassification Results")

leaf_lda_confusion <- confusionMatrix(leaf_lda_tidy$Predicted, leaf_lda_tidy$Actual)

kable(
  leaf_lda_confusion$table
  ,caption = "LDA Confusion Matrix"
)

kable(
  leaf_lda_confusion$byClass
  ,caption = "LDA Confusion Matrix Stats"
)
```

Tables 5-6 are the results from the LDA, in this model six leaves were classifieds, which included three pear and three cherry. Upon examining where these leaves are situated in the scatter plot these leaves are along the boundary lines. 

In the confusion matrix, out of the 12 pear leaves 9 were classified correctly and 3 were classifieds and out of the 16 cherry leaves 13 were classified correctly and 3 were misclassified. 

The sensitivity represents the proportion of predicted pear leaves that were actually pear leaves, which was `r round(leaf_lda_confusion$byClass[["Sensitivity"]], 4)`. The specificity represents the proportion of predicted cherry leaves that were actually cherry leaves, which was `r round(leaf_lda_confusion$byClass[["Specificity"]], 4)`.

```{r LDA ROC}
leaf_lda_tidy %>% 
  roc(
    Actual ~ Cherry
    ,data = .
  ) %>% 
  ggroc() +
  labs(
    title = "Figure 02: ROC Curve"
    ,subtitle = "Based on the LDA Model"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

The Receiver Operating Characteristic (ROC) Curve represents the matched pairs of Specificity and Sensitivity at different threshold levels. What this means is that for a given data point  we can assign a leaf type based on the estimated probability. We choose a threshold for this assignment, for example anything with a probability of being a cherry leaf of $\geq 0.60$ we would assign a predicted type of cherry, with a threshold of 60\%.

It is important to have an Area Under the Curve (AUC) that approaches one, as this ensures that the ROC curve approaches 1 for both specificity and sensitivity. For the LDA, we have a AUC of `r round(roc(Actual ~ Cherry, data = leaf_lda_tidy)$auc, 4)`, which is pretty good.

## New Data
```{r New Data LDA}
leaf_lda_newdata <- predict(leaf_lda, newdata = select(leaf_test, Length, Width))

leaf_lda_newdata$class <- as.vector(leaf_lda_newdata$class)
leaf_lda_newdata$posterior <- as_tibble(leaf_lda_newdata$posterior)

tibble(
  Predicted = factor(leaf_lda_newdata$class, levels = c("Pear", "Cherry"))
  ,Cherry = leaf_lda_newdata$posterior$Cherry
  ,Pear = leaf_lda_newdata$posterior$Pear
) %>% 
  bind_cols(leaf_test) %>% 
  kable(caption = "LDA New Data Predictions")
```

Table 8 represents the predicted lead type based on data that was not originally included in the data set. 

## Observation Space
```{r Obs Space LDA}
leaf_lda_tidy %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Predicted
    )
  ) +
  geom_polygon(
    data =
      leaf_lda_tidy %>%
      split(.$Predicted) %>%
      map(~ select(., Width, Length)) %>%
      map(~ chull(.)) %>%
      map(as_tibble) %>%
      bind_rows(.id = "Predicted") %>%
      mutate(Predicted = factor(Predicted, levels = c("Pear", "Cherry"))) %>%
      inner_join(
        leaf_lda_tidy
        ,by =
          c(
            "value" = "Number By Predicted"
            ,"Predicted" = "Predicted"
          )
      )
    ,alpha = 0.1
  ) +
  geom_point(
    aes(shape = Actual)
    ,size = 2
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 03: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull Based on the LDA Predicted Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```
The convex hull in Figure 03 represents the region that captures all the points of a given leaf type, and is convex in nature. The convexity ensures that any linear combination of points in the set is still in the set. We can evaluate this on the predicted types given by the LDA to see the separating hyper plane between the two convex sets, as this gives a good approximation of the line used to differentiate between the types by the LDA. As well, by looking at the combination of shape and colour, we can see which points were misclassified as per Table 3.

```{r Obs Space Sample}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Type
    )
  ) + 
  geom_polygon(
    data =
      leaf_data %>%
      split(.$Type) %>% 
      map(~ select(., Width, Length)) %>%
      map_df(~ chull(.)) %>% 
      gather(
        key = Type
        ,value = `Number By Type`
      ) %>% 
      mutate(Type = factor(Type, levels = c("Pear", "Cherry"))) %>% 
      left_join(leaf_data, by = c("Number By Type", "Type"))
    ,alpha = 0.1
  ) +
  geom_point() +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 04: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull of that Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

This is the convex hall of the raw data, and as you can see there is an overlap which indicates that there is no strict separation in the raw data and therefore it was necessary to conduct the LDA. 

# Probability Distributions
## Contour
```{r Contour Type}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Type
    )
  ) + 
  geom_density_2d() +
  geom_point(
    data = 
      leaf_data %>% 
      select(-Type)
    ,colour="grey92"
    ,alpha = 0.9
  ) +
  geom_point() +
  facet_wrap(
    ~ Type
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 05: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Contour Plot"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

Contour plots show the clustering of data for pear and cherry trees. Each contour line represents the same density anywhere along that line. As the contour line density increases so does the steepness of the graph and the probability that a given leaf will have those characteristics. 

In Figure 05, there are two disctrint shapes for the contour plot of pear and cherry leaves, this is due to the different covariance matrices of the leaves as seen in Table 10 and 11. The pear leaves tended to have a more similar length and width whereas, the cherry leaves tended to have a longer length and a skinnier width.  

```{r Contour Combined}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
    )
  ) + 
  geom_density_2d(colour = "black") +
  geom_point(aes(colour = Type)) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 06: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with a Contour Plot of that Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
    ,colour = "Type"
  )
```

Figure 6 is the combined contour plot of the raw data. In this contour plot there is potential bimodality, as seen with the two peaks in the contour plot. However, these peaks fall along the diagonal and not along the vertical or horizontal axises, which indcates that the bimodality is shared between the length and the width.   

```{r Density Type}
leaf_data %>% 
  select(Type, Length, Width) %>% 
  gather(
    key = Dimension
    ,value = Measurement
    ,-Type
  ) %>% 
  ggplot(aes(x = Measurement, y = ..density.., colour = Type)) +
  geom_histogram(
    alpha = 0.5
    ,binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3))
  ) +
  geom_density() +
  facet_grid(
    Dimension ~ Type
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(title = "Figure 07: Density Plot by Type")
```



```{r Density Combined}
leaf_data %>% 
  select(Length, Width) %>% 
  gather(
    key = Dimension
    ,value = Measurement
  ) %>% 
  ggplot(aes(x = Measurement, y = ..density..)) +
  geom_histogram(
    alpha = 0.5
    ,binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3))
  ) +
  geom_density() +
  facet_wrap(
    ~ Dimension
  ) +
  labs(title = "Figure 08: Density Plot")
```

Figure 8 shows the bimodality of the data for both the length and the width. The bimodaility of the data is more prominent in the length than in the width, which becomes especially apparent when you split it up by type. 

## Covariance Matrix
```{r Covariance Matrix Combined}
leaf_data %>% 
  select(Length, Width) %>% 
  cov() %>% 
  kable(caption = "Shared Covariance Matrix")
```

```{r Covariance Matrix Seperate}
leaf_cov_cherry <- 
  leaf_data %>% 
  filter(Type == "Cherry") %>% 
  select(Length, Width) %>% 
  cov() 
leaf_cov_cherry %>% 
  kable(caption = "Cherry Covariance Matrix")

leaf_cov_pear <-
  leaf_data %>% 
  filter(Type == "Pear") %>% 
  select(Length, Width) %>% 
  cov() 
leaf_cov_pear%>% 
  kable(caption = "Pear Covariance Matrix")

sqrt(sum((leaf_cov_cherry - leaf_cov_pear)^2))
```



# Classification Procedure (QDA)
The difference between LDA and Quadratic Discriminat Analysis (QDA) is that QDA doesn't rely on the assumption that both classes of data share a covariance matrix, which is a crucial assumption in LDA. This allows us to perform analysis on data where this assumption may not hold in exchange for an increased variance. As well, it doesn't require the classification rule to be linear, but instead can be a quadratic function.

## Training Data
```{r QDA}
leaf_qda <- 
  leaf_data %>%
  qda(
    Type ~ Length + Width
    ,data = .
    ,cv = TRUE
  )

leaf_qda$prior %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  kable(
    digits = 4
    ,col.names = c("Type", "Probability")
    ,caption = "QDA Prior Probabilities"
  )

leaf_qda$mean %>% 
  kable(
    digits = 4
    ,caption = "QDA Group Means"
  )


leaf_qda_pred <- predict(leaf_qda, newdata = select(leaf_data, Length, Width))

leaf_qda_pred$class <- as.vector(leaf_qda_pred$class)
leaf_qda_pred$posterior <- as_tibble(leaf_qda_pred$posterior)

leaf_qda_tidy <-
  tibble(
    Predicted = factor(leaf_qda_pred$class, levels = c("Pear", "Cherry"))
    ,Cherry = leaf_qda_pred$posterior$Cherry
    ,Pear = leaf_qda_pred$posterior$Pear
  ) %>% 
  bind_cols(leaf_data) %>% 
  rename(
    Actual = Type
    ,`Number By Actual` = `Number By Type`
  ) %>%
  group_by(Predicted) %>% 
  mutate(
    `Number By Predicted` = 1:length(Predicted)
  ) %>% 
  ungroup()
```

The prior probability of being a Pear leaf is `r round(leaf_qda$prior[["Pear"]], 4)`. The prior probability of being a Cherry leaf is `r round(leaf_qda$prior[["Cherry"]], 4)`. 

```{r QDA Results}
leaf_qda_tidy %>% 
  select(
    Predicted
    ,Actual
    ,Length
    ,Width
    ,`Cherry Probability` = Cherry
    ,`Pear Probability` = Pear
  ) %>% 
  mutate(`Correct Prediction` = if_else(Predicted == Actual, TRUE, FALSE)) %>% 
  filter(!`Correct Prediction`) %>% 
  kable(digits = 4, caption = "QDA Misclassification Results")

leaf_qda_confusion <- confusionMatrix(leaf_lda_tidy$Predicted, leaf_lda_tidy$Actual)

kable(
  leaf_qda_confusion$table
  ,caption = "QDA Confusion Matrix"
)

kable(
  leaf_qda_confusion$byClass
  ,caption = "QDA Confusion Matrix Stats"
)
```

Tables 12-14 are the results from the QDA, in this model six leaves were classifieds, which included three pear and three cherry. 

In the confusion matrix, out of the 12 pear leaves 9 were classified correctly and 3 were classifieds and out of the 16 cherry leaves 13 were classified correctly and 3 were misclassified. 

The sensitivity represents the proportion of predicted pear leaves that were actually pear leaves, which was `r round(leaf_qda_confusion$byClass[["Sensitivity"]], 4)`. The specificity represents the proportion of predicted cherry leaves that were actually cherry leaves, which was `r round(leaf_qda_confusion$byClass[["Specificity"]], 4)`.


## New Data
```{r New Data QDA}
leaf_qda_newdata <- predict(leaf_qda, newdata = select(leaf_test, Length, Width))

leaf_qda_newdata$class <- as.vector(leaf_qda_newdata$class)
leaf_qda_newdata$posterior <- as_tibble(leaf_qda_newdata$posterior)

tibble(
  Predicted = factor(leaf_qda_newdata$class, levels = c("Pear", "Cherry"))
  ,Cherry = leaf_qda_newdata$posterior$Cherry
  ,Pear = leaf_qda_newdata$posterior$Pear
) %>% 
  bind_cols(leaf_test) %>% 
  kable(caption = "QDA New Data Predictions")
```

Table 17 represents the predicted lead type based on data that was not originally included in the data set. 

```{r QDA ROC}
leaf_qda_tidy %>% 
  roc(
    Actual ~ Cherry
    ,data = .
  ) %>% 
  ggroc() +
  labs(
    title = "Figure O9: ROC Curve"
    ,subtitle = "Based on the QDA Model"
  )
```

For the QDA, we have a AUC of `r round(roc(Actual ~ Cherry, data = leaf_qda_tidy)$auc, 4)`, which is better than our LDA model.

## Observation Space
```{r Obs Space QDA}
leaf_qda_tidy %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Predicted
    )
  ) +
  geom_polygon(
    data =
      leaf_qda_tidy %>%
      split(.$Predicted) %>%
      map(~ select(., Width, Length)) %>%
      map(~ chull(.)) %>%
      map(as_tibble) %>%
      bind_rows(.id = "Predicted") %>%
      mutate(Predicted = factor(Predicted, levels = c("Pear", "Cherry"))) %>%
      inner_join(
        leaf_qda_tidy
        ,by =
          c(
            "value" = "Number By Predicted"
            ,"Predicted" = "Predicted"
          )
      )
    ,alpha = 0.1
  ) +
  geom_point(
    aes(shape = Actual)
    ,size = 2
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure XX: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull Based on the QDA Predicted Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

The convex hulls look identical to that described in the LDA section, due to the fact that the classification was identical between the two models.

SCOTTTTTTTTTTY


# Classification Procedure (GLM)
## Training Data
```{r GLM}
leaf_train_control <- trainControl(method = "LOOCV")
leaf_logit <-
  leaf_data %>%
  select(Type, Length, Width) %>%
  train(
    Type ~ Length + Width
    ,data = .
    ,trControl = leaf_train_control
    ,method = "glm"
  )

leaf_logit %>% 
  summary() %>% 
  pander()

leaf_logit_pred <- 
  list(
    class = predict(leaf_logit, type = "raw")
    ,posterior = predict(leaf_logit, type = "prob")
  )

leaf_logit_pred$class <- as.vector(leaf_logit_pred$class)
leaf_logit_pred$posterior <- as_tibble(leaf_logit_pred$posterior)

leaf_logit_tidy <-
  tibble(
    Predicted = factor(leaf_logit_pred$class, levels = c("Pear", "Cherry"))
    ,Cherry = leaf_logit_pred$posterior$Cherry
    ,Pear = leaf_logit_pred$posterior$Pear
  ) %>% 
  bind_cols(leaf_data) %>% 
  rename(
    Actual = Type
    ,`Number By Actual` = `Number By Type`
  ) %>%
  group_by(Predicted) %>% 
  mutate(
    `Number By Predicted` = 1:length(Predicted)
  ) %>% 
  ungroup()
```

```{r GLM Results}
leaf_logit_tidy %>% 
  select(
    Predicted
    ,Actual
    ,Length
    ,Width
    ,`Cherry Probability` = Cherry
    ,`Pear Probability` = Pear
  ) %>% 
  mutate(`Correct Prediction` = if_else(Predicted == Actual, TRUE, FALSE)) %>% 
  filter(!`Correct Prediction`) %>% 
  kable(digits = 4, caption = "Logit Misclassification Results")

leaf_logit_confusion <- confusionMatrix(leaf_lda_tidy$Predicted, leaf_lda_tidy$Actual)

kable(
  leaf_logit_confusion$table
  ,caption = "Logit Confusion Matrix"
)

kable(
  leaf_logit_confusion$byClass
  ,caption = "Logit Confusion Matrix Stats"
)
```

```{r GLM ROC}
leaf_data %>% 
  bind_cols(tibble(Prob = predict(leaf_logit, type = "prob")$Cherry)) %>% 
  roc(
    Type ~ Prob
    ,data = .
  ) %>% 
  ggroc() +
  labs(
    title = "Figure XX: ROC Curve"
    ,subtitle = "Based on the Logit Model"
  )
```


## New Data
```{r New Data GLM}
tibble(
  Predicted = predict(object = leaf_logit, newdata = leaf_test, type = "raw")
  ,`Cherry Probability` = predict(object = leaf_logit, newdata = leaf_test, type = "prob")$Cherry
  ,`Pear Probability` = predict(object = leaf_logit, newdata = leaf_test, type = "prob")$Pear
) %>% 
  bind_cols(leaf_test) %>% 
  kable(caption = "Logit New Data Predictions")
```


## Observation Space
```{r Obs Space GLM}
leaf_logit_tidy %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Predicted
    )
  ) +
  geom_polygon(
    data =
      leaf_logit_tidy %>%
      split(.$Predicted) %>%
      map(~ select(., Width, Length)) %>%
      map(~ chull(.)) %>%
      map(as_tibble) %>%
      bind_rows(.id = "Predicted") %>%
      mutate(Predicted = factor(Predicted, levels = c("Pear", "Cherry"))) %>%
      inner_join(
        leaf_qda_tidy
        ,by =
          c(
            "value" = "Number By Predicted"
            ,"Predicted" = "Predicted"
          )
      )
    ,alpha = 0.1
  ) +
  geom_point(
    aes(shape = Actual)
    ,size = 2
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 09: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull Based on the Logit Predicted Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```



# Conclusion

Based on these models, the best model is the LDA, which offers the simplest model for ease of interpretation. All the model perform at a similar level of accurracy, in terms of their precision, specificity, and sensitvity, and AUC. The LDA model is a relatively simple model to interpret as it has a intuitive geometric interpretation that is accessible to those with a limited statistics background. As such it is our recommended model to use for this task.


# Appendix
## Appendix A
```{r Appendix A}
leaf_data %>% 
  select(
    `Number By Type`
    ,Type
    ,Length
    ,Width
  ) %>% 
  kable(caption = "Data")
```

# References
[1] The Parts of a Leaf. (17, October 30). Retrieved March 20, 18, from http://www.robinsonlibrary.com/science/botany/anatomy/leafparts.htm

[2] Britannica, T. E. (2016, November 11). Cherry. Retrieved March 20, 2018, from https://www.britannica.com/plant/cherry

[3] Britannica, T. E. (2015, May 13). Pear. Retrieved March 20, 2018, from https://www.britannica.com/plant/pear
