---
title: "Classification of Cherry and Pear Leaves by Length and Width"
author: "Kaisa Roggeveen, Scott Graham"
date: "March 22nd 2018"
header-includes:
  - \newcommand{\Prob}{\operatorname{P}}
  - \newcommand{\E}{\operatorname{E}}
  - \newcommand{\Var}{\operatorname{Var}}
  - \newcommand{\Cov}{\operatorname{Cov}}
  - \newcommand{\se}{\operatorname{se}}
  - \newcommand{\re}{\operatorname{re}}
  - \newcommand{\ybar}{{\overline{Y}}}
  - \newcommand{\phat}{{\hat{p}}}
  - \newcommand{\that}{{\hat{T}}}
  - \newcommand{\med}{{\tilde{Y}}}
  - \newcommand{\logit}{{\operatorname{Logit}}}
output: 
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(pander, warn.conflicts = FALSE, quietly = TRUE)
library(knitr, warn.conflicts = FALSE, quietly = TRUE)
library(MASS, warn.conflicts = FALSE, quietly = TRUE)
library(pROC, warn.conflicts = FALSE, quietly = TRUE)
library(klaR, warn.conflicts = FALSE, quietly = TRUE)
library(caret, warn.conflicts = FALSE, quietly = TRUE)
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE)
library(magrittr, warn.conflicts = FALSE, quietly = TRUE)
library(ggfortify, warn.conflicts = FALSE, quietly = TRUE)

theme_minimal2 <- theme_minimal() %>%  theme_set()
theme_minimal2 <-
  theme_update(
    panel.border = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
    ,strip.background = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
  )

# Functions ----------
geom_cor <- function(data, ...){
  data %>%
    filter_all(any_vars(!is.na(.))) %>% 
    cor(...) %>% 
    as.data.frame() %>%  
    rownames_to_column() %>% 
    as.tibble() %>% 
    gather(
      key = Column
      ,value = Correlation
      ,-rowname
    ) %>% 
    rename(Row = rowname) %>% 
    ggplot(
      aes(
        x = Column
        ,y = Row
        ,fill = Correlation
      )
    ) +
    geom_raster() +
    scale_fill_distiller(
      type = "div"
      ,palette = "RdBu"
      ,limits = c(-1, 1)
    ) +
    theme(
      axis.text.x = element_text(angle = 90, hjust = 1)
      ,axis.title.x = element_blank()
      ,axis.title.y = element_blank()
      ,panel.grid = element_blank()
      ,panel.background = element_blank()
    )
}

ggroc <- function(roc, showAUC = TRUE, interval = 0.2, breaks = seq(0, 1, interval)){
  require(pROC)
  if(class(roc) != "roc") simpleError("Please provide roc object from pROC package")
  
  plot_data <- 
    data.frame(
      plotx <- rev(roc$specificities)
      ,ploty <- rev(roc$sensitivities)
    )
  
  plot_data %>% 
    ggplot(
      aes(
        x = plotx
        ,y = ploty
      )
    ) +
    geom_segment(
      aes(
        x = 0
        ,y = 1
        ,xend = 1
        ,yend = 0
      )
      ,alpha = 0.5
    ) + 
    geom_step() +
    scale_x_reverse(
      name = "Specificity"
      ,limits = c(1, 0)
      ,breaks = breaks
      ,expand = c(0.001,0.001)
    ) + 
    scale_y_continuous(
      name = "Sensitivity"
      ,limits = c(0, 1)
      ,breaks = breaks
      ,expand = c(0.001, 0.001)
    ) +
    coord_equal() + 
    annotate(
      geom = "text"
      ,x = 0.05 + interval/2
      ,y = interval/2
      ,vjust = 0
      ,label = paste("AUC =", sprintf("%.3f",roc$auc))
    )
}

# Data Import ----------
leaf_data <- 
  "../Data/leaf_data.csv" %>% 
  read_csv() %>% 
  mutate(Type = as.factor(Type))

leaf_data$Type <- factor(leaf_data$Type, levels = c("Pear", "Cherry"))

measurements <- c("Length", "Width")

leaf_test <-
  tibble(
    Number = 1:3
    ,Length = c(8.2, 5.2, 7.6)
    ,Width = c(3.2, 3.8, 4.0)
  )
```
Dr. Steven M. Vamosi

Associate Dean, Diversity, Equity and Inclusion

Professor, Population Biology

2500 University Drive NW

Department of Biological Sciences

University of Calgary

Calgary AB

T2N 1N4 Canada

# Introduction
The intent of this paper is to develop a method for classifying leaves as either Cherry or Pear, based on their measured length and width. This method was developed for Dr. Steven Vamosi, a botanist from the University of Calgary. 

The classification method used was Linear Discriminant Analysis (LDA), developed by R.A Fischer. In order to ensure models have strong predictive power, Leave One Out Cross Validation (LOOCV) was used for all the models described in this paper. In order to determine the accuracy and usefulness of our models we used LOOCV to compare a predicted classification vs. the actual classification in order to determine if there are any misclassifications.

Cherry and Pear leaves are both leaves from fruit trees. Cherry trees belong to the genus Prunus and Pear trees belong to the genus Pyrus [2],[3]. A common feature amongst the leaves is that they both have a midrib, which is the central vein of the leaf which extends along the leaf's center line. 


# Data
## Measurement Process
The first step taken in the measurement of the leaves was to give each leaf an identification number based on the species. The method used to measure the dimensions was to create a box with the minimum length and width in which the entire leaf would be encompassed in the box. 

To begin creating the sides of the box, a ruler was aligned parallel to the midrib, which is the central vein in the leaf and moved towards the left and the right of the picture until only one point on the leaf remained [1]. From the single point on the side of the leaf, a line was drawn parallel to the midrib of the leaf. 

Next, the base and point of the leaf were measured, a ruler was placed perpendicular to the midrib and the ruler was moved towards to tip of the leaf until a single point remained, a line was draw perpendicular to the midrib at this point. At the base of the leaves the length of the leaf was set as the point where the leaf ends and the stem begins, at this point a line was drawn perpendicular to the midrib. 

After all the boxes were created, the width (lines parallel to midrib) and the length (lines perpendicular to midrib) were measured and the results were recorded in a spread sheet. 

## Data Creation
```{r Data Creation 01}
leaf_data %>% 
  select(-starts_with("Number")) %>% 
  group_by(Type) %>% 
  summary() %>% 
  kable(caption = "Data Summary")
```

In this original data set there are a few issues that need to be acknowledged. The first issues that occurred during the data measurements was the result of the leaves that were distributed as the training sample were images, in which the images were not to scale. This resulted in a few outlines, which much larger lengths and widths compared to the other leaves in the set. These outlines included Pear#12, Cherry#10 and Cherry#5. However, based on the nature of this project in just observing the ratio between the length and width, this should not be affected by the size of the image, unless the image was stretched in either direction.

```{r Scatter Plot}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Type
      ,label = `Number By Type`
    )
  ) + 
  geom_text() +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 01: Length vs Width Scatter Plot"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```
In Figure 01 there is a distinct separation in the data of the cherry and the pear. As mentioned above, the outlines are Cherry#10, Cherry#5 and Pear#12 these outlines appear to follow a similar grouping and therefore they were kept in the data set. The raw data is located in Appendix A.  

# Classification Procedure (LDA)
## Training Data
```{r LDA}
leaf_lda <- 
  leaf_data %>%
  lda(
    Type ~ Length + Width
    ,data = .
    ,cv = TRUE
  )

leaf_lda$prior %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  kable(
    digits = 4
    ,col.names = c("Type", "Probability")
    ,caption = "LDA Prior Probabilities"
  )

leaf_lda$mean %>% 
  kable(
    digits = 4
    ,caption = "LDA Group Means"
  )

leaf_lda$scaling %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  kable(
    digits = 4
    ,col.names = c("Dimension", "Coefficient")
    ,caption = "LDA Coefficients of Linear Discriminants"
  )

leaf_lda_pred <- predict(leaf_lda, newdata = select(leaf_data, Length, Width))

leaf_lda_pred$class <- as.vector(leaf_lda_pred$class)
leaf_lda_pred$posterior <- as_tibble(leaf_lda_pred$posterior)
leaf_lda_pred$x <- as.vector(leaf_lda_pred$x)

leaf_lda_tidy <-
  tibble(
    Predicted = factor(leaf_lda_pred$class, levels = c("Pear", "Cherry"))
    ,Cherry = leaf_lda_pred$posterior$Cherry
    ,Pear = leaf_lda_pred$posterior$Pear
    ,LD1 = leaf_lda_pred$x
  ) %>% 
  bind_cols(leaf_data) %>% 
  rename(
    Actual = Type
    ,`Number By Actual` = `Number By Type`
  ) %>%
  group_by(Predicted) %>% 
  mutate(
    `Number By Predicted` = 1:length(Predicted)
  ) %>% 
  ungroup()
```

Tables 2-4 represent the output of a LDA done on the raw data. Table 2 represents the prior probabilities of falling in a particular type. The prior probability of being a Pear leaf is `r round(leaf_lda$prior[["Pear"]], 4)`. The prior probability of being a Cherry leaf is `r round(leaf_lda$prior[["Cherry"]], 4)`. 

The LDA coefficients act similar to those in a ordinary least squares model/regression model, where the sum product of those with a row of data gives us a score for that leaf. That score is then transformed into an estimated probability for both types of leaves, using Bayesian methodology. The LDA coefficient for Length is `r leaf_lda$scaling["Length", "LD1"]`, and for Width it is `r leaf_lda$scaling["Width", "LD1"]`.

```{r LDA Results}
leaf_lda_tidy %>% 
  select(
    Predicted
    ,Actual
    ,Length
    ,Width
    ,`Cherry Probability` = Cherry
    ,`Pear Probability` = Pear
  ) %>% 
  mutate(`Correct Prediction` = if_else(Predicted == Actual, TRUE, FALSE)) %>% 
  filter(!`Correct Prediction`) %>% 
  kable(digits = 4, caption = "LDA Misclassification Results")

leaf_lda_confusion <- confusionMatrix(leaf_lda_tidy$Predicted, leaf_lda_tidy$Actual)

leaf_lda_confusion$table %>% 
  as.matrix() %>%  
  as.data.frame() %>% 
  rename(Actual = Reference) %>% 
  spread(
    key = Actual
    ,value = Freq
  ) %>% 
  kable(caption = "LDA Confusion Matrix")

kable(
  leaf_lda_confusion$byClass
  ,caption = "LDA Confusion Matrix Stats"
)
```

Tables 5-6 are the results from the LDA, in this model six leaves were misclassified, which included three pear and three cherry. Upon examining where these leaves are situated in the scatter plot these leaves are along the boundary lines. 

In the confusion matrix, out of the 12 pear leaves 9 were classified correctly and 3 were misclassified and out of the 16 cherry leaves 13 were classified correctly and 3 were misclassified. 

The sensitivity represents the proportion of predicted pear leaves that were actually pear leaves, which was `r round(leaf_lda_confusion$byClass[["Sensitivity"]], 4)`. The specificity represents the proportion of predicted cherry leaves that were actually cherry leaves, which was `r round(leaf_lda_confusion$byClass[["Specificity"]], 4)`.

```{r LDA ROC}
leaf_lda_roc <- 
  leaf_lda_tidy %>% 
  roc(
    Actual ~ Cherry
    ,data = .
  )

leaf_lda_roc %>% 
  ggroc() +
  labs(
    title = "Figure 02: ROC Curve"
    ,subtitle = "Based on the LDA Model"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

The Receiver Operating Characteristic (ROC) Curve represents the matched pairs of Specificity and Sensitivity at different threshold levels. What this means is that for a given data point  we can assign a leaf type based on the estimated probability. To do this, we choose a threshold for this assignment, for example anything with a probability of being a cherry leaf of $\geq 0.60$ we would assign a predicted type of cherry, with a threshold of 60\%. To determine the best threshold to use in terms of maximizing both the specificity and sensitivity, we find the point on the ROC curve which is furthest away from the line splitting the graph along the diagonal. The best threshold to use as determined by the ROC Curve is `r round(coords(roc = leaf_lda_roc, x = "best", ret = "threshold"), 4)`. 

It is important to have an Area Under the Curve (AUC) that approaches one, as this ensures that the ROC curve approaches 1 for both specificity and sensitivity. For the LDA, we have a AUC of `r round(roc(Actual ~ Cherry, data = leaf_lda_tidy)$auc, 4)`, which is pretty good.

## New Data
```{r New Data LDA}
leaf_lda_newdata <- predict(leaf_lda, newdata = select(leaf_test, Length, Width))

leaf_lda_newdata$class <- as.vector(leaf_lda_newdata$class)
leaf_lda_newdata$posterior <- as_tibble(leaf_lda_newdata$posterior)

tibble(
  Predicted = factor(leaf_lda_newdata$class, levels = c("Pear", "Cherry"))
  ,Cherry = leaf_lda_newdata$posterior$Cherry
  ,Pear = leaf_lda_newdata$posterior$Pear
) %>% 
  bind_cols(leaf_test) %>% 
  kable(caption = "LDA New Data Predictions")
```

Table 8 represents the predicted lead type based on data that was not originally included in the data set. 

## Observation Space
```{r Obs Space LDA}
leaf_lda_tidy %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Predicted
    )
  ) +
  geom_polygon(
    data =
      leaf_lda_tidy %>%
      split(.$Predicted) %>%
      map(~ select(., Width, Length)) %>%
      map(~ chull(.)) %>%
      map(as_tibble) %>%
      bind_rows(.id = "Predicted") %>%
      mutate(Predicted = factor(Predicted, levels = c("Pear", "Cherry"))) %>%
      inner_join(
        leaf_lda_tidy
        ,by =
          c(
            "value" = "Number By Predicted"
            ,"Predicted" = "Predicted"
          )
      )
    ,alpha = 0.1
  ) +
  geom_point(
    aes(shape = Actual)
    ,size = 2
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 03a: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull Based on the LDA Predicted Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )

partimat(
  Type ~ Length + Width
  ,data = leaf_data
  ,method = "lda"
  ,main = "Figure 03b: LDA Partition Plot"
  ,col.wrong = "black"
  ,image.colors = c("#66c2a5", "#fc8d62")
  ,name = c("Length (cm)", "Width (cm)")
)
```

The convex hull in Figure 03a represents the region that captures all the points of a given leaf type, and is convex in nature. The convexity ensures that any linear combination of points in the set is still in the set. We can evaluate this on the predicted types given by the LDA to see the separating hyper plane between the two convex sets, as this gives a good approximation of the line used to differentiate between the types by the LDA. As well, by looking at the combination of shape and colour, we can see which points were misclassified as per Table 3.

Figure 03b shows the classification line used by the LDA. As expected it is a linear classification rule, which splits the observation space into two half spaces.

```{r Obs Space Sample}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Type
    )
  ) + 
  geom_polygon(
    data =
      leaf_data %>%
      split(.$Type) %>% 
      map(~ select(., Width, Length)) %>%
      map_df(~ chull(.)) %>% 
      gather(
        key = Type
        ,value = `Number By Type`
      ) %>% 
      mutate(Type = factor(Type, levels = c("Pear", "Cherry"))) %>% 
      left_join(leaf_data, by = c("Number By Type", "Type"))
    ,alpha = 0.1
  ) +
  geom_point() +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 04: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull of that Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

This is the convex hall of the raw data, and as you can see there is an overlap which indicates that there is no strict separation in the raw data and therefore it was necessary to conduct the LDA. 

# Probability Distributions
## Contour
```{r Contour Type}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Type
    )
  ) + 
  geom_density_2d() +
  geom_point(
    data = 
      leaf_data %>% 
      select(-Type)
    ,colour="grey92"
    ,alpha = 0.9
  ) +
  geom_point() +
  facet_wrap(
    ~ Type
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 05: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Contour Plot"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

Contour plots show the clustering of data for pear and cherry trees. Each contour line represents the same density anywhere along that line. As the contour line density increases so does the steepness of the graph and the probability that a given leaf will have those characteristics. 

In Figure 05, there are two distinct shapes for the contour plot of pear and cherry leaves, this is due to the different covariance matrices of the leaves as seen in Table 10 and 11. The pear leaves tended to have a more similar length and width whereas, the cherry leaves tended to have a longer length and a skinnier width.  

```{r Contour Combined}
leaf_data %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
    )
  ) + 
  geom_density_2d(colour = "black") +
  geom_point(aes(colour = Type)) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 06: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with a Contour Plot of that Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
    ,colour = "Type"
  )
```

Figure 6 is the combined contour plot of the raw data. In this contour plot there is potential bimodality, as seen with the two peaks in the contour plot. However, these peaks fall along the diagonal and not along the vertical or horizontal axes, which indicates that the bimodality is shared between the length and the width.   

```{r Density Type}
leaf_data %>% 
  select(Type, Length, Width) %>% 
  gather(
    key = Dimension
    ,value = Measurement
    ,-Type
  ) %>% 
  ggplot(aes(x = Measurement, y = ..density.., colour = Type)) +
  geom_histogram(
    alpha = 0.5
    ,binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3))
  ) +
  geom_density() +
  facet_grid(
    Dimension ~ Type
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(title = "Figure 07: Density Plot by Type")
```



```{r Density Combined}
leaf_data %>% 
  select(Length, Width) %>% 
  gather(
    key = Dimension
    ,value = Measurement
  ) %>% 
  ggplot(aes(x = Measurement, y = ..density..)) +
  geom_histogram(
    alpha = 0.5
    ,binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3))
  ) +
  geom_density() +
  facet_wrap(
    ~ Dimension
  ) +
  labs(title = "Figure 08: Density Plot")
```

Figure 8 shows the bimodality of the data for both the length and the width. The bimodality of the data is more prominent in the length than in the width, which becomes especially apparent when you split it up by type. 

## Covariance Matrix
```{r Covariance Matrix Combined}
leaf_cov_combined <- 
  leaf_data %>% 
  select(Length, Width) %>% 
  cov()
leaf_cov_combined %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename(` ` = rowname) %>% 
  kable(caption = "Shared Covariance Matrix")
```

```{r Covariance Matrix Seperate}
leaf_cov_cherry <- 
  leaf_data %>% 
  filter(Type == "Cherry") %>% 
  select(Length, Width) %>% 
  cov() 
leaf_cov_cherry %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename(` ` = rowname) %>% 
  kable(caption = "Cherry Covariance Matrix")

leaf_cov_pear <-
  leaf_data %>% 
  filter(Type == "Pear") %>% 
  select(Length, Width) %>% 
  cov() 
leaf_cov_pear %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  rename(` ` = rowname) %>% 
  kable(caption = "Pear Covariance Matrix")

leaf_cov_combined_cherry_l2 <- 
  sqrt(sum((leaf_cov_combined - leaf_cov_cherry)^2)) / mean(c(sum(dim(leaf_cov_combined)), sum(dim(leaf_cov_cherry))))

leaf_cov_combined_pear_l2 <- 
  sqrt(sum((leaf_cov_combined - leaf_cov_pear)^2)) / mean(c(sum(dim(leaf_cov_combined)), sum(dim(leaf_cov_pear))))

leaf_cov_cherry_pear_l2 <- 
  sqrt(sum((leaf_cov_cherry - leaf_cov_pear)^2)) / mean(c(sum(dim(leaf_cov_cherry)), sum(dim(leaf_cov_pear))))
```

In order to get an idea of how different the split by type covariance matrices are, we calculated the average distance between entries. This is effectively the $\ell_{2}$-norm divided by the number of entries in the covariance matrix.

$$
  \text{Average Distance} =
  \frac{|| \Sigma_{\text{Cherry}} - \Sigma_{\text{Pear}} ||_{2}}{r \times c} = 
  \frac{\sqrt{\sum_{i = 1}^{2}\sum_{j = 1}^{2} \left( \sigma_{\text{Cherry}, i, j} - \sigma_{\text{Pear}, i, j} \right)^{2}}}{4}
$$

Which comes out to `r round(leaf_cov_cherry_pear_l2, 4)`. We can also compute this by comparing the shared and cherry covariance matrix, giving us `r round(leaf_cov_combined_cherry_l2, 4)`, and for the shared and pear covariance matrix, giving us `r round(leaf_cov_combined_pear_l2, 4)`. As one would expect, the difference between the individual covariance matrices and the shared one is smaller than the difference between the two individual covariance matrices, due to the pooling property of the shared matrix. Because of this, it is safe to assume that the shared covariance matrix accurately estimates the individual covariance matrix for the two types of leaves.


# Classification Procedure (QDA)
The difference between LDA and Quadratic Discriminant Analysis (QDA) is that QDA doesn't rely on the assumption that both classes of data share a covariance matrix, which is a crucial assumption in LDA. This allows us to perform analysis on data where this assumption may not hold in exchange for an increased variance. As well, it doesn't require the classification rule to be linear, but instead can be a quadratic function.

## Training Data
```{r QDA}
leaf_qda <- 
  leaf_data %>%
  qda(
    Type ~ Length + Width
    ,data = .
    ,cv = TRUE
  )

leaf_qda$prior %>% 
  as.data.frame() %>% 
  rownames_to_column() %>% 
  kable(
    digits = 4
    ,col.names = c("Type", "Probability")
    ,caption = "QDA Prior Probabilities"
  )

leaf_qda$mean %>% 
  kable(
    digits = 4
    ,caption = "QDA Group Means"
  )


leaf_qda_pred <- predict(leaf_qda, newdata = select(leaf_data, Length, Width))

leaf_qda_pred$class <- as.vector(leaf_qda_pred$class)
leaf_qda_pred$posterior <- as_tibble(leaf_qda_pred$posterior)

leaf_qda_tidy <-
  tibble(
    Predicted = factor(leaf_qda_pred$class, levels = c("Pear", "Cherry"))
    ,Cherry = leaf_qda_pred$posterior$Cherry
    ,Pear = leaf_qda_pred$posterior$Pear
  ) %>% 
  bind_cols(leaf_data) %>% 
  rename(
    Actual = Type
    ,`Number By Actual` = `Number By Type`
  ) %>%
  group_by(Predicted) %>% 
  mutate(
    `Number By Predicted` = 1:length(Predicted)
  ) %>% 
  ungroup()
```

The prior probability of being a Pear leaf is `r round(leaf_qda$prior[["Pear"]], 4)`. The prior probability of being a Cherry leaf is `r round(leaf_qda$prior[["Cherry"]], 4)`. 

```{r QDA Results}
leaf_qda_tidy %>% 
  select(
    Predicted
    ,Actual
    ,Length
    ,Width
    ,`Cherry Probability` = Cherry
    ,`Pear Probability` = Pear
  ) %>% 
  mutate(`Correct Prediction` = if_else(Predicted == Actual, TRUE, FALSE)) %>% 
  filter(!`Correct Prediction`) %>% 
  kable(digits = 4, caption = "QDA Misclassification Results")

leaf_qda_confusion <- confusionMatrix(leaf_lda_tidy$Predicted, leaf_lda_tidy$Actual)

leaf_qda_confusion$table %>% 
  as.matrix() %>%  
  as.data.frame() %>% 
  rename(Actual = Reference) %>% 
  spread(
    key = Actual
    ,value = Freq
  ) %>% 
  kable(caption = "QDA Confusion Matrix")

kable(
  leaf_qda_confusion$byClass
  ,caption = "QDA Confusion Matrix Stats"
)
```

Tables 12-14 are the results from the QDA, in this model six leaves were misclassified, which included three pear and three cherry. 

In the confusion matrix, out of the 12 pear leaves 9 were classified correctly and 3 were misclassified and out of the 16 cherry leaves 13 were classified correctly and 3 were misclassified. 

The sensitivity represents the proportion of predicted pear leaves that were actually pear leaves, which was `r round(leaf_qda_confusion$byClass[["Sensitivity"]], 4)`. The specificity represents the proportion of predicted cherry leaves that were actually cherry leaves, which was `r round(leaf_qda_confusion$byClass[["Specificity"]], 4)`.

```{r QDA ROC}
leaf_qda_roc <- 
  leaf_qda_tidy %>% 
  roc(
    Actual ~ Cherry
    ,data = .
  )

leaf_qda_roc %>% 
  ggroc() +
  labs(
    title = "Figure 09: ROC Curve"
    ,subtitle = "Based on the QDA Model"
  )
```

For the QDA, we have a AUC of `r round(roc(Actual ~ Cherry, data = leaf_qda_tidy)$auc, 4)`, which is better than our LDA model. The best threshold to use as determined by the ROC Curve is `r round(coords(roc = leaf_qda_roc, x = "best", ret = "threshold"), 4)`.


## New Data
```{r New Data QDA}
leaf_qda_newdata <- predict(leaf_qda, newdata = select(leaf_test, Length, Width))

leaf_qda_newdata$class <- as.vector(leaf_qda_newdata$class)
leaf_qda_newdata$posterior <- as_tibble(leaf_qda_newdata$posterior)

tibble(
  Predicted = factor(leaf_qda_newdata$class, levels = c("Pear", "Cherry"))
  ,Cherry = leaf_qda_newdata$posterior$Cherry
  ,Pear = leaf_qda_newdata$posterior$Pear
) %>% 
  bind_cols(leaf_test) %>% 
  kable(caption = "QDA New Data Predictions")
```

Table 17 represents the predicted leaf type based on data that was not originally included in the data set. These results are similar to what was seen in the LDA model.

## Observation Space
```{r Obs Space QDA}
leaf_qda_tidy %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Predicted
    )
  ) +
  geom_polygon(
    data =
      leaf_qda_tidy %>%
      split(.$Predicted) %>%
      map(~ select(., Width, Length)) %>%
      map(~ chull(.)) %>%
      map(as_tibble) %>%
      bind_rows(.id = "Predicted") %>%
      mutate(Predicted = factor(Predicted, levels = c("Pear", "Cherry"))) %>%
      inner_join(
        leaf_qda_tidy
        ,by =
          c(
            "value" = "Number By Predicted"
            ,"Predicted" = "Predicted"
          )
      )
    ,alpha = 0.1
  ) +
  geom_point(
    aes(shape = Actual)
    ,size = 2
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 10a: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull Based on the QDA Predicted Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )

partimat(
  Type ~ Length + Width
  ,data = leaf_data
  ,method = "qda"
  ,main = "Figure 10b: QDA Partition Plot"
  ,col.wrong = "black"
  ,image.colors = c("#66c2a5", "#fc8d62")
  ,name = c("Length (cm)", "Width (cm)")
)
```

The convex hulls in Figure 10a look identical to that described in the LDA section, due to the fact that the classification was identical between the two models. However when looking at Figure 10b, we can see that the model is using a curved or quadratic classification rule, which doesn't allow us to split the observation space into two half spaces, due to a lack of linearity.

# Classification Procedure (GLM)
The final classification method we looked at was using a cross-validated logistic regression model to determine the predicted type of leaf. This is based on a regression of the predicted log-odds of falling particular class on the predictors of length and width.

## Training Data
```{r GLM}
leaf_train_control <- trainControl(method = "LOOCV")
leaf_logit <-
  leaf_data %>%
  select(Type, Length, Width) %>%
  train(
    Type ~ Length + Width
    ,data = .
    ,trControl = leaf_train_control
    ,method = "glm"
  )

leaf_logit %>% 
  summary() %>% 
  pander()

leaf_logit_pred <- 
  list(
    class = predict(leaf_logit, type = "raw")
    ,posterior = predict(leaf_logit, type = "prob")
  )

leaf_logit_pred$class <- as.vector(leaf_logit_pred$class)
leaf_logit_pred$posterior <- as_tibble(leaf_logit_pred$posterior)

leaf_logit_tidy <-
  tibble(
    Predicted = factor(leaf_logit_pred$class, levels = c("Pear", "Cherry"))
    ,Cherry = leaf_logit_pred$posterior$Cherry
    ,Pear = leaf_logit_pred$posterior$Pear
  ) %>% 
  bind_cols(leaf_data) %>% 
  rename(
    Actual = Type
    ,`Number By Actual` = `Number By Type`
  ) %>%
  group_by(Predicted) %>% 
  mutate(
    `Number By Predicted` = 1:length(Predicted)
  ) %>% 
  ungroup()
```

Through the usage of a LOOCV Logistic regression, we derived the following model:
$$
  \logit(\Prob(\text{Cherry} | L, W)) =
  -1.74 + 0.7764L - 0.9338W + \epsilon
$$

Both of the slope variables were statistically significant at $\alpha=0.05$. From this, the estimated odds in favor of being a cherry leaf vs. a pear leaf increases by a multiplicative factor of `r round(exp(leaf_logit$finalModel$coefficients[["Length"]]), 4)` for every centimeter increase in length. The estimated odds in favor of being a cherry leaf vs. a pear leaf increases by a multiplicative factor of `r round(exp(leaf_logit$finalModel$coefficients[["Width"]]), 4)` for every centimeter increase in width.

```{r GLM Results}
leaf_logit_tidy %>% 
  select(
    Predicted
    ,Actual
    ,Length
    ,Width
    ,`Cherry Probability` = Cherry
    ,`Pear Probability` = Pear
  ) %>% 
  mutate(`Correct Prediction` = if_else(Predicted == Actual, TRUE, FALSE)) %>% 
  filter(!`Correct Prediction`) %>% 
  kable(digits = 4, caption = "Logit Misclassification Results")

leaf_logit_confusion <- confusionMatrix(leaf_lda_tidy$Predicted, leaf_lda_tidy$Actual)

leaf_logit_confusion$table %>% 
  as.matrix() %>%  
  as.data.frame() %>% 
  rename(Actual = Reference) %>% 
  spread(
    key = Actual
    ,value = Freq
  ) %>% 
  kable(caption = "Logit Confusion Matrix")
kable(
  leaf_logit_confusion$byClass
  ,caption = "Logit Confusion Matrix Stats"
)
```

Tables 20-22 are the results from the logistic regression, in this model six leaves were misclassified, which included three pear and three cherry. 

In the confusion matrix, out of the 12 pear leaves 9 were classified correctly and 3 were misclassified and out of the 16 cherry leaves 13 were classified correctly and 3 were misclassified. 

The sensitivity represents the proportion of predicted pear leaves that were actually pear leaves, which was `r round(leaf_logit_confusion$byClass[["Sensitivity"]], 4)`. The specificity represents the proportion of predicted cherry leaves that were actually cherry leaves, which was `r round(leaf_logit_confusion$byClass[["Specificity"]], 4)`.

```{r GLM ROC}
leaf_logit_roc <- 
  leaf_data %>% 
  bind_cols(tibble(Prob = predict(leaf_logit, type = "prob")$Cherry)) %>% 
  roc(
    Type ~ Prob
    ,data = .
  )

leaf_logit_roc %>% 
  ggroc() +
  labs(
    title = "Figure 11: ROC Curve"
    ,subtitle = "Based on the Logit Model"
  )
```
For the logistic regression, we have a AUC of `r round(roc(Actual ~ Cherry, data = leaf_logit_tidy)$auc, 4)`, which is worse than our LDA model. The best threshold to use as determined by the ROC Curve is `r round(coords(roc = leaf_logit_roc, x = "best", ret = "threshold"), 4)`.

## New Data
```{r New Data GLM}
tibble(
  Predicted = predict(object = leaf_logit, newdata = leaf_test, type = "raw")
  ,`Cherry Probability` = predict(object = leaf_logit, newdata = leaf_test, type = "prob")$Cherry
  ,`Pear Probability` = predict(object = leaf_logit, newdata = leaf_test, type = "prob")$Pear
) %>% 
  bind_cols(leaf_test) %>% 
  kable(caption = "Logit New Data Predictions")
```

Table 23 represents the predicted leaf type based on data that was not originally included in the data set. These results are similar to what was seen in both the LDA, and QDA models.


## Observation Space
```{r Obs Space GLM}
leaf_logit_tidy %>% 
  ggplot(
    aes(
      x = Width
      ,y = Length
      ,colour = Predicted
    )
  ) +
  geom_polygon(
    data =
      leaf_logit_tidy %>%
      split(.$Predicted) %>%
      map(~ select(., Width, Length)) %>%
      map(~ chull(.)) %>%
      map(as_tibble) %>%
      bind_rows(.id = "Predicted") %>%
      mutate(Predicted = factor(Predicted, levels = c("Pear", "Cherry"))) %>%
      inner_join(
        leaf_qda_tidy
        ,by =
          c(
            "value" = "Number By Predicted"
            ,"Predicted" = "Predicted"
          )
      )
    ,alpha = 0.1
  ) +
  geom_point(
    aes(shape = Actual)
    ,size = 2
  ) +
  scale_colour_brewer(
    type = "qual"
    ,palette = "Set2"
  ) +
  labs(
    title = "Figure 12: Length vs Width Scatter Plot"
    ,subtitle = "Overlayed with the Convex Hull Based on the Logit Predicted Type"
    ,x = "Width (cm)"
    ,y = "Length (cm)"
  )
```

The convex hulls in Figure 12 look identical to the ones presented in the LDA and QDA methods. This is because the logistic regression came up with the same classifications as the other two models.

# Conclusion

Based on these models, the best model is the LDA, which offers the simplest model for ease of interpretation. All the model perform at a similar level of accuracy, in terms of their precision, specificity, and sensitivity, and AUC. The LDA model is a relatively simple model to interpret as it has a intuitive geometric interpretation that is accessible to those with a limited statistics background. As such it is our recommended model to use for this task.

```{r New Data Conclusion}
tibble(
  `LDA Predicted` = factor(leaf_lda_newdata$class, levels = c("Pear", "Cherry"))
  ,`LDA Cherry Probability` = leaf_lda_newdata$posterior$Cherry %>% round(4)
  ,`LDA Pear Probability` = leaf_lda_newdata$posterior$Pear %>% round(4)
  ,`QDA Predicted` = factor(leaf_qda_newdata$class, levels = c("Pear", "Cherry"))
  ,`QDA Cherry Probability` = leaf_qda_newdata$posterior$Cherry %>% round(4)
  ,`QDA Pear Probability` = leaf_qda_newdata$posterior$Pear %>% round(4)
  ,`Logit Predicted` = predict(object = leaf_logit, newdata = leaf_test, type = "raw")
  ,`Logit Cherry Probability` = predict(object = leaf_logit, newdata = leaf_test, type = "prob")$Cherry %>% round(4)
  ,`Logit Pear Probability` = predict(object = leaf_logit, newdata = leaf_test, type = "prob")$Pear %>% round(4)
) %>% 
  bind_cols(leaf_test) %>%
  gather(
    key = Model
    ,value = Prediction
    ,-c(
      Number
      ,Length
      ,Width
    )
  ) %>% 
  kable()
```

For the new data, Table 24 shows the results across all 3 models. The logistic regression and LDA were quite similar in terms of predicted probabilities. The QDA model's probabilities were closer to the prior probabilities described in the raw data.

# Appendix
## Appendix A
```{r Appendix A}
leaf_data %>% 
  select(
    `Number By Type`
    ,Type
    ,Length
    ,Width
  ) %>% 
  kable(caption = "Data")
```

# References
[1] The Parts of a Leaf. (17, October 30). Retrieved March 20, 18, from http://www.robinsonlibrary.com/science/botany/anatomy/leafparts.htm

[2] Britannica, T. E. (2016, November 11). Cherry. Retrieved March 20, 2018, from https://www.britannica.com/plant/cherry

[3] Britannica, T. E. (2015, May 13). Pear. Retrieved March 20, 2018, from https://www.britannica.com/plant/pear
