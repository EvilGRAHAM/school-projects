---
title: "Snow"
author: "Kaisa Roggeveen, Scott Graham"
date: "April 6th, 2018"
header-includes:
  - \newcommand{\Prob}{\operatorname{P}}
  - \newcommand{\E}{\operatorname{E}}
  - \newcommand{\Var}{\operatorname{Var}}
  - \newcommand{\Cov}{\operatorname{Cov}}
  - \newcommand{\se}{\operatorname{se}}
  - \newcommand{\re}{\operatorname{re}}
  - \newcommand{\ybar}{{\overline{Y}}}
  - \newcommand{\phat}{{\hat{p}}}
  - \newcommand{\that}{{\hat{T}}}
  - \newcommand{\med}{{\tilde{Y}}}
  - \newcommand{\logit}{{\operatorname{Logit}}}
output: 
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(pander, warn.conflicts = FALSE, quietly = TRUE)
library(knitr, warn.conflicts = FALSE, quietly = TRUE)
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE)
library(ggfortify, warn.conflicts = FALSE, quietly = TRUE)

theme_minimal2 <- theme_minimal() %>%  theme_set()
theme_minimal2 <-
  theme_update(
    panel.border = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
    ,strip.background = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
  )

# Data Import ----------
snow_wide <- 
  "../Data/snow_data.csv" %>% 
  read_csv()

snow_long <- 
  snow_wide %>% 
  gather(
    key = Gauge
    ,value = Gain
    ,-Density
  ) %>% 
  mutate(Gauge = as.factor(Gauge))

alpha <- 0.05
```

# Introduction
## Data
```{r Wide Data}
snow_wide %>% 
  kable(caption = "Wide Data")
```

```{r Measured Gain by Gauge}
snow_long %>% 
  ggplot(
    aes(
      x = Gauge
      ,y = log(Gain)
      ,colour = Density
    )
  ) +
  geom_point() +
  facet_grid(
    ~ Gauge
    ,scales = "free_x"
  ) +
  scale_colour_distiller(
    type = "seq"
    ,palette = "OrRd"
    ,direction = 1
  ) +
  labs(
    title = "Figure 01: Measured Gain by Gauge"
    ,colour = expression(paste("Density (g/cm"^3,")"))
  ) +
  theme(
    axis.text.x = element_blank()
    ,axis.title.x = element_blank()
    ,legend.position = "bottom"
  )
```

Talk about how the gauges are consistent

```{r Training Data}
snow_long_train <- 
  snow_long %>%
  filter(Gauge %in% c("G 01", "G 02", "G 03"))

snow_long_valid <-
  snow_long %>%
  filter(!(Gauge %in% c("G 01", "G 02", "G 03")))

snow_long_train %>% 
  ggplot(
    aes(
      x = Density
      ,y = log(Gain)
    )
  ) +
  geom_smooth(method = "lm") +
  geom_point() +
  labs(
    title = "Log Gain vs. Density with Training Data"
    ,x = expression(paste("Density (g/cm"^3,")"))
  )
```

talk about how a log model is appropriate


## Training vs. Validation Data

Talk about how we split the data into training data (Gauges 1-3), and validation data (gauges 4-10). 

# Calibration
## Classic Calibration Method
For the classic calibration method, we regress our measurement ($G:=$ Gain) as a function of the known variable ($D:=$ Density).
```{r LM}
snow_lm <-
  snow_long_train %>% 
  lm(
      log(Gain) ~ Density
    ,data = .
  )

snow_lm %>% 
  summary() %>% 
  pander()
```

From this we take our linear regression model, and invert it, solving for the known predictor variable $D$. This gives us:

$$
  \hat{D_{i}} = 
  -\frac{\ln(G_{i}) - 6.0032 - \epsilon_{i}}{4.6301} =
  1.2965(1+\epsilon_{i}) - 0.2160\ln(G_{i}),
  \epsilon_{i} \stackrel{iid}{\sim} \mathcal{N}(0,\sigma^{2})
$$

From this, we can come up with both the point estimates for $D$, and a prediction interval, using:
$$
  \se\left( \hat{D_{i}} \right) =
  \frac{\sqrt{MSE}}{\hat{\beta_{1}}}\sqrt{1 + \frac{1}{n} + \frac{\left( D_{i}-\bar{D} \right)^{2}}{S_{DD}}},
  MSE = 0.0028,
  \bar{D} = 0.3311\dots,
  S_{DD} = 0.2293
$$

As well as assuming an underlying Student's t-distribution, with $df=n-p=n-2=27-2=25$.

```{r Classic Calibration Est}
classic_calibration <- function(x, object){
  (log(x) - object$coefficients[["(Intercept)"]]) / object$coefficients[["Density"]]
}

snow_summ_valid_cc <-
  snow_long_valid %>% 
  group_by(Density) %>% 
  summarize(`Mean Gain` = mean(Gain)) %>% 
  mutate(
    `Est Density` = classic_calibration(x = `Mean Gain`, object = snow_lm)
    ,`Prediction Std. Error` = 
      sqrt(
        (summary(snow_lm)$sigma/snow_lm$coefficients[[2]])^2 *
        ( 1 + 
            1/nrow(snow_long_train) + 
            (`Est Density` - mean(snow_long_train$Density))^2/sd(snow_long_train$Density) )
      )
    ,`Prediction LB` = `Est Density` - qt(1-alpha/2, snow_lm$df.residual, lower.tail = TRUE) * `Prediction Std. Error`
    ,`Prediction UB` = `Est Density` + qt(1-alpha/2, snow_lm$df.residual, lower.tail = TRUE) * `Prediction Std. Error`
  )
snow_summ_valid_cc %>% 
  kable()
```

From this, we can plot the results, with the Estimated Density on the y-axis, and the Mean Gain for a given Density on the x-axis. The grey band represents the 95\% Prediction Interval for $D$.

```{r CC Dens vs Gain}
snow_summ_valid_cc %>% 
  ggplot(
    aes(
      x = `Mean Gain`
      ,y = `Est Density`
    )
  ) +
  geom_ribbon(
    aes(
      ymin = `Prediction LB`
      ,ymax = `Prediction UB`
    )
    ,fill = "grey60"
    ,alpha = 0.4
  ) +
  stat_function(
    fun = classic_calibration
    ,args = list(object = snow_lm)
  ) +
  geom_point() +
  labs(
    title = "Estimated Density vs. Gain"
    ,subtitle = "With Prediction Interval and Curve"
    ,y = expression(paste("Estimated Density (g/cm"^3,")"))
  )
```

Parker et al. (2010), provides a method for finding a finding the bias in the estimation of $\hat{D_{i}}$:
$$
  \operatorname{bias}\left( \hat{D_{i}} \right) =
  \frac{(D_{i} - \bar{D})MSE}{\hat{\beta_{1}}^{2}S_{DD}}
$$

```{r CC Unbias}
snow_summ_unbias_cc <- 
  snow_summ_valid_cc %>% 
  mutate(
    Bias = 
      (Density - mean(snow_long_train$Density)) * summary(snow_lm)$sigma^2 /
      (snow_lm$coefficients[[2]]^2 * sd(snow_long_train$Density))
    ,`Unbiased Est Density` = `Est Density` - Bias
  ) %>% 
  select(Density, `Est Density`, Bias, `Unbiased Est Density`)
snow_summ_unbias_cc %>% 
  kable(caption = "Unbiasing the Estimated Density for Classic Calibration")

snow_summ_unbias_cc %>% 
  ggplot(
    aes(
      x = Density
      ,y = Bias
    )
  ) +
  geom_line() +
  labs(
    title = "Rate of Change of Bias"
    ,x = expression(paste("Density (g/cm"^3,")"))
    ,y = expression(paste("Estimated Bias (g/cm"^3,")"))
  )
```


## Inverse Regression
The other methodology looked at was using a inverse regression technique, by using $D$ as the dependent variable, and $G$ as the independent variable. This should give us a slightly different result than the coefficients calculated under the classical calibration method, as regression equations don't invert exactly unless the correlation between the two variables is $\pm 1$. Because our correlation (`r cor(snow_long_train$Density, snow_long_train$Gain)`) is close to $-1$, the coefficients will be very close. For this we use the model:

$$
  \hat{D_{i}} =
  \hat{\gamma_{0}} + \hat{\gamma_{1}}\left( \ln(G_{i}) - \overline{\ln(G_{i})} \right) + \epsilon_{i} =
  0.3311 - 02155\left( \ln(G_{i}) - 4.4701 \right) + \epsilon_{i},
  \epsilon_{i} \stackrel{iid}{\sim} \mathcal{N}(0,\sigma^{2})
$$
$$
  \overline{\ln(G)} = \sum_{i=1}^{n}\frac{\ln{G_{i}}}{n},
  \hat{\gamma_{0}} = \bar{D}
$$

```{r Inv LM}
snow_inv_lm <-
  snow_long_train %>% 
  mutate(`Centred Gain` = exp(log(Gain) - mean(log(Gain)))) %>% 
  lm(
    Density ~ log(`Centred Gain`)
    ,data = .
  )
snow_inv_lm %>% 
  summary() %>% 
  pander()
```

As we can see from the regression output, $\hat{\gamma_{0}} = \bar{D}$, which is what we want. We then estimated $\hat{D_{i}}$, and the prediction interval estimate:

```{r Inverse Reg Est}
inverse_regression <- function(x, object){
  predict(object = object, newdata = tibble(`Centred Gain` = exp(log(x) - mean(log(snow_long_train$Gain)))))
}

snow_summ_valid_ir <-
  snow_long_valid %>% 
  group_by(Density) %>% 
  summarize(
    `Mean Gain` = mean(Gain)
  ) %>% 
  mutate(
    `Mean Centred Gain` = exp(log(`Mean Gain`) - mean(log(snow_long_train$Gain)))
  ) %>% 
  mutate(
    `Est Density` = inverse_regression(x = `Mean Gain`, object = snow_inv_lm)
    ,`Prediction Std. Error` =
      sqrt(
        (summary(snow_inv_lm)$sigma)^2 *
          ( 1 +
              1/nrow(snow_long_train) +
              (log(`Mean Gain`) - mean(log(snow_long_train$Gain)))^2/sd(log(snow_long_train$Gain)) )
      )
    ,`Prediction LB` = `Est Density` - qt(1-alpha/2, snow_inv_lm$df.residual, lower.tail = TRUE) * `Prediction Std. Error`
    ,`Prediction UB` = `Est Density` + qt(1-alpha/2, snow_inv_lm$df.residual, lower.tail = TRUE) * `Prediction Std. Error`
  )
snow_summ_valid_ir %>% 
  kable()
```

As well as recreating the plot used in the classical calibration method. Note that the plot looks very similar, with a marginally large standard error term.

```{r IR Dens vs Gain}
snow_summ_valid_ir %>% 
  ggplot(
    aes(
      x = `Mean Gain`
      ,y = `Est Density`
    )
  ) +
  geom_ribbon(
    aes(
      ymin = `Prediction LB`
      ,ymax = `Prediction UB`
    )
    ,fill = "grey60"
    ,alpha = 0.4
  ) +
  stat_function(
    fun = inverse_regression
    ,args = list(object = snow_inv_lm)
  ) +
  geom_point() +
  labs(
    title = "Estimated Density vs. Gain"
    ,subtitle = "With Prediction Interval and Curve"
    ,y = expression(paste("Estimated Density (g/cm"^3,")"))
  )
```

Parker et al. (2010), provides a means of unbiasing $\hat{D_{i}}$ for inverse regression too:

$$
  \operatorname{bias}\left( \hat{D_{i}} \right) =
  \frac{\bar{D} - D_{i}}{1 + \frac{\hat{\beta_{1}}^{2}S_{DD}}{(n-1)MSE}}
$$

This is then plotted against $D$, to see the rate of change.

```{r IR Unbias}
snow_summ_unbias_ir <- 
  snow_summ_valid_ir %>% 
  mutate(
    Bias = 
      (mean(snow_long_train$Density) - Density) /
      (1 + 
         (snow_inv_lm$coefficients[[2]]^2*sd(snow_long_train$Density))/
         ((nrow(snow_long_train) - 1)*summary(snow_inv_lm)$sigma^2)
      )
    ,`Unbiased Est Density` = `Est Density` - Bias
  ) %>% 
  select(Density, `Est Density`, Bias, `Unbiased Est Density`)
snow_summ_unbias_ir %>% 
  kable(caption = "Unbiasing the Estimated Density for Inverse Regression")

snow_summ_unbias_ir %>% 
  ggplot(
    aes(
      x = Density
      ,y = Bias
    )
  ) +
  geom_line() +
  labs(
    title = "Rate of Change of Bias"
    ,x = expression(paste("Density (g/cm"^3,")"))
    ,y = expression(paste("Estimated Bias (g/cm"^3,")"))
  )
```


## Comparison
```{r Comparison}
snow_summ_unbias_cc %>% 
  select(
    Density
    ,`CC Est Density` = `Est Density`
    ,`CC Bias` = Bias
  ) %>% 
  left_join(
    snow_summ_unbias_ir %>% 
      select(
        Density
        ,`IR Est Density` = `Est Density`
        ,`IR Bias` = Bias
      )
    ,by = "Density"
  ) %>% 
  kable(caption = "Comparison of Bias")

snow_summ_valid_cc %>% 
  select(
    Density
    ,`Mean Gain`
    ,`CC Est Density` = `Est Density`
    ,`CC Prediction Std. Error` = `Prediction Std. Error`
  ) %>% 
  left_join(
    snow_summ_valid_ir %>% 
      select(  
        Density
        ,`Mean Gain`
        ,`IR Est Density` = `Est Density`
        ,`IR Prediction Std. Error` = `Prediction Std. Error`
      )
    ,by = c("Density" = "Density", "Mean Gain" = "Mean Gain")
  ) %>% 
  kable(caption = "Comparison of Standard Error")
```

To compare the two methods, we looked at the size of their Bias, and the size of the Standard Errors. From the two tables above, one can see that the Classic Calibration method outperformed the Inverse Regression by having both a smaller estimated bias, and a smaller estimated standard error based on the same training and validation samples.

## Measurement Error
If we were to assume that the given densities for the polyethylene blocks contained small amounts of measurement error, this change the size of our interval estimates.

Let:
$$
  \hat{D_{i}} =
  D_{i} + \epsilon_{D,i},
  \epsilon_{D,i} \sim \mathcal{N}(0, \sigma^{2}) \implies
$$
$$
  \ln(G_{i}) = 
  \hat{\beta_{0}} + \hat{\beta_{1}}\hat{D_{i}} + \epsilon_{G,i} =
  \hat{\beta_{0}} + \hat{\beta_{1}}(D_{i} + \epsilon_{D,i}) + \epsilon_{G,i} =
$$
$$
  \hat{\beta_{0}} + \hat{\beta_{1}}D_{i} + (\hat{\beta_{1}}\epsilon_{D,i} + \epsilon_{G,i}) =
  \hat{\beta_{0}} + \hat{\beta_{1}}D_{i} + \epsilon^{\star}_{G,i}
$$
Where
$$
  \epsilon^{\star}_{G,i} \sim
  \mathcal{N}\left( 0, \hat{\beta_{1}}^{2}\sigma_{D}^{2}+\sigma_{G}^{2}+2\hat{\beta_{1}}\Cov(D,G) \right)
$$

Now hopefully the covariance term is equal to 0, otherwise additional issues would arise in the calibration. While this won't affect the coefficient estimation done in the regressions, it would affect the size of the interval estimates, by increasing them to reflect the greater uncertainty in the quality of measurements.